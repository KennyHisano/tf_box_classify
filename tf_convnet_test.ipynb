{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Convolutional Neural Network (CNN) to Classify Images\n",
    "\n",
    "Source code here:\n",
    "https://github.com/wwoo/tf_box_classify\n",
    "\n",
    "This is a simple example demonstrating how to classify images from a webcam using TensorFlow.  The model is a convolutional neural network (CNN) that classifies images of a box into four classes (upright, tilted, open and spilled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper parameters to use for training\n",
    "TRAIN_BATCH_SIZE = 10\n",
    "TRAIN_EPOCHS = 5\n",
    "VALID_BATCH_SIZE = 40\n",
    "VALID_EPOCHS = None\n",
    "SHUFFLE_BATCHES = True\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_CLASSES = 4\n",
    "KEEP_PROB = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image parameters\n",
    "IMAGE_SIZE = 150\n",
    "IMAGE_RESIZE_FACTOR = 1\n",
    "IMAGE_CHANNELS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a sample of our training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(Image(filename='../images/normal_processed/normal-0.jpg'))\n",
    "display(Image(filename='../images/normal_processed/normal-5.jpg'))\n",
    "display(Image(filename='../images/normal_processed/normal-10.jpg'))\n",
    "display(Image(filename='../images/normal_processed/normal-15.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at an excerpt of our labeled training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../train.txt', 'r') as f:\n",
    "    for i in range(0, 20):\n",
    "        print(f.readline().rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our validation dataset is similarly labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../valid.txt', 'r') as f:\n",
    "    for i in range(0, 20):\n",
    "        print(f.readline().rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use TensorFlow input queues to read our training and validation data.  Labeled training data is read from `train.txt`, and validation data from `valid.txt`.\n",
    "\n",
    "`get_batch_inputs` to retrieve the next `TRAIN_BATCH_SIZE` batch of labeled data during training and validation. \n",
    "\n",
    "Assuming images are 150 x 150 pixels (single channel / grayscale) and using a batch size of 10, `get_batch_inputs` returns two values:\n",
    "\n",
    "1. A 4-D tensor of shape [150, 150, 1, 10], which is a batch training images\n",
    "2. A 2-D tensor of shape [4, 10], which is the corresponding batch of labels represented as \"one-hot\" vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_inputs(train_file, batch_size=TRAIN_BATCH_SIZE, num_epochs=TRAIN_EPOCHS):\n",
    "    image_list, label_list = get_image_label_list(train_file)\n",
    "    input_queue = tf.train.slice_input_producer([image_list, label_list],\n",
    "        num_epochs=num_epochs, shuffle=SHUFFLE_BATCHES)\n",
    "    image, label = read_image_from_disk(input_queue)\n",
    "    image = tf.reshape(image, [IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS])\n",
    "    image_batch, label_batch = tf.train.batch([image, label],\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    return preprocess_images(image_batch), tf.one_hot(tf.to_int64(label_batch),\n",
    "        NUM_CLASSES, on_value=1.0, off_value=0.0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define a few helper functions called by `get_batch_inputs`: \n",
    "\n",
    "`get_image_label_list` to read our text files into lists. \n",
    "\n",
    "`preprocess_images` to resize our images and convert them to grayscale.\n",
    "\n",
    "`read_image_from_disk` to convert JPEG images on disk to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_image_from_disk(input_queue):\n",
    "    label = input_queue[1]\n",
    "    file_contents = tf.read_file(input_queue[0])\n",
    "    rgb_image = tf.image.decode_jpeg(file_contents, channels=IMAGE_CHANNELS, name=\"decode_jpeg\")  \n",
    "    \n",
    "    return rgb_image, label\n",
    "\n",
    "\n",
    "def preprocess_images(image_batch, resize_factor=IMAGE_RESIZE_FACTOR):\n",
    "    gs_image_batch = tf.image.rgb_to_grayscale(image_batch, name=\"rgb_to_grayscale\")\n",
    "    new_image_size = int(round(IMAGE_SIZE / resize_factor))\n",
    "\n",
    "    return tf.image.resize_images(image_batch, new_image_size, new_image_size)\n",
    "\n",
    "def get_image_label_list(image_label_file):\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    for line in open(image_label_file, \"r\"):\n",
    "        filename, label = line[:-1].split(' ')\n",
    "        filenames.append(filename)\n",
    "        labels.append(int(label))\n",
    "\n",
    "    print \"get_image_label_list: read \" + str(len(filenames)) + \" items.\"\n",
    "    \n",
    "    return filenames, labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some helper functions for convolution and max pooling.\n",
    "\n",
    "`conv2d` combines a convolution with a stride of 1. We're using `padding='SAME'`, so the input and output tensor will the same shape. We then apply the ReLU activation function to the output tensor.\n",
    "\n",
    "`maxpool2d` performs max pool sampling, which will half the size of the input tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`conv_net` builds most of the CNN specific parts of our TensorFlow graph. \n",
    "\n",
    "Each layer of the CNN performs a convolution, ReLU activation and down sampling, which is then fed as input to the next CNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases, image_size, keep_prob=KEEP_PROB):\n",
    "    x = tf.reshape(x, shape=[-1, image_size, image_size, 1])\n",
    "\n",
    "    # Convolution and max pooling layers\n",
    "    # Each max pooling layer reduces dimensionality by 2\n",
    "\n",
    "    with tf.name_scope('layer1'):\n",
    "        # Convolution and max pooling layer 1\n",
    "        conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "        conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    with tf.name_scope('layer2'):\n",
    "        # Convolution and max pooling layer 2\n",
    "        conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "        conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    with tf.name_scope('layer3'):\n",
    "        # Convolution and max pooling layer 3\n",
    "        conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
    "        conv3 = maxpool2d(conv3, k=2)\n",
    "\n",
    "    with tf.name_scope('layer4'):\n",
    "        # Convolution and max pooling layer 4\n",
    "        conv4 = conv2d(conv3, weights['wc4'], biases['bc4'])\n",
    "        conv4 = maxpool2d(conv4, k=2)\n",
    "\n",
    "    with tf.name_scope('fully_connected'):\n",
    "        # Fully-connected layer\n",
    "        fc1 = tf.reshape(conv4, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "        fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "        # Apply dropout\n",
    "        fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "    with tf.name_scope('output'):\n",
    "        # Output, class prediction\n",
    "        out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purely for debugging and educational purposes, generate a summary of (one) first layer convolution and activation.  These can be viewed in Tensorboard, which is a tool for visualising TensorFlow graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_image_summary(x, weights, biases, step, image_size=IMAGE_SIZE):\n",
    "    x = tf.reshape(x, shape=[-1, image_size, image_size, 1])\n",
    "\n",
    "    with tf.name_scope('generate_image_summary'):\n",
    "        x = tf.nn.conv2d(x, weights['wc1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        x = tf.nn.bias_add(x, biases['bc1'])\n",
    "        x_slice = tf.slice(x, [0, 0, 0, 0], [TRAIN_BATCH_SIZE, image_size, image_size, 1])\n",
    "        conv_summary = tf.image_summary(\"img_conv_{:05d}\".format(step), x_slice, max_images=1)\n",
    "        relu_summary = tf.image_summary(\"img_relu_{:05d}\".format(step), tf.nn.relu(x_slice), max_images=1)\n",
    "\n",
    "    return conv_summary, relu_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all our helper functions defined, we're ready to start building our TensorFlow graph.\n",
    "\n",
    "Firstly, create input nodes to retrieve image batches. We'll define two batches:\n",
    "\n",
    "`train_image_batch` and `train_label_batch` to get the next batch of training images and corresponding labels.  These are fed into `x_` and `y_` placeholders for each step of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read inventory of training images and labels\n",
    "with tf.name_scope('batch_inputs'):\n",
    "    train_file = \"../train.txt\"\n",
    "    valid_file = \"../valid.txt\"\n",
    "\n",
    "    train_image_batch, train_label_batch = get_batch_inputs(train_file, batch_size=TRAIN_BATCH_SIZE, \n",
    "                                                            num_epochs=TRAIN_EPOCHS)\n",
    "    \n",
    "    valid_image_batch, valid_label_batch = get_batch_inputs(valid_file, batch_size=VALID_BATCH_SIZE, \n",
    "                                                            num_epochs=VALID_EPOCHS)\n",
    "    \n",
    "    image_size = IMAGE_SIZE / IMAGE_RESIZE_FACTOR\n",
    "    \n",
    "    # These are image and label batch placeholders which we'll feed in during training\n",
    "    x_ = tf.placeholder(\"float32\", shape=[None, image_size, image_size, IMAGE_CHANNELS])\n",
    "    y_ = tf.placeholder(\"float32\", shape=[None, NUM_CLASSES])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define weights and biases for each of our convolution layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store weights for our convolution & fully-connected layers\n",
    "with tf.name_scope('weights'):\n",
    "    weights = {\n",
    "        # 5x5 conv, 1 input, 32 outputs\n",
    "        'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "        # 5x5 conv, 32 inputs, 64 outputs\n",
    "        'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "        # 5x5 conv, 64 inputs, 128 outputs\n",
    "        'wc3': tf.Variable(tf.random_normal([5, 5, 64, 128])),\n",
    "        # 5x5 conv, 128 inputs, 256 outputs\n",
    "        'wc4': tf.Variable(tf.random_normal([5, 5, 128, 256])),\n",
    "        # fully connected, 10*10*256 inputs, 1024 outputs\n",
    "        'wd1': tf.Variable(tf.random_normal([10*10*256, 1024])),\n",
    "        # 1024 inputs, 4 class labels (prediction)\n",
    "        'out': tf.Variable(tf.random_normal([1024, NUM_CLASSES]))\n",
    "    }\n",
    "    \n",
    "# Store biases for our convolution and fully-connected layers\n",
    "with tf.name_scope('biases'):\n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.random_normal([32])),\n",
    "        'bc2': tf.Variable(tf.random_normal([64])),\n",
    "        'bc3': tf.Variable(tf.random_normal([128])),\n",
    "        'bc4': tf.Variable(tf.random_normal([256])),\n",
    "        'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "        'out': tf.Variable(tf.random_normal([NUM_CLASSES]))\n",
    "    }    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping some of our activations will help prevent over-fitting on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define dropout rate to prevent overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our weights, biases and dropout probability, we can call `conv_net` to run our current batch input through the convolution, maxpool and fully-connected layers.\n",
    "\n",
    "The output from the fully-connected layer is a set of probability distributions across our `NUM_CLASSES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build our graph\n",
    "pred = conv_net(x_, weights, biases, image_size, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now measure the \"loss\" between our predicted probability distributions and the true distributions from our one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate loss\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y_))\n",
    "    cost_summary = tf.scalar_summary(\"cost_summary\", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a training step to minimise the loss between the predicted and true distribution.  TensorFlow keeps track of the whole graph and performs auto-differentation at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run optimizer step\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we'll define some steps to check the accuracy of our training.  We'll call this every n steps and write out a summary for visualisation via Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model accuracy\n",
    "with tf.name_scope('predict'):\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    accuracy_summary = tf.scalar_summary(\"accuracy_summary\", accuracy)\n",
    "    w_summary = tf.histogram_summary(\"weights\", weights['wc1'])\n",
    "    b_summary = tf.histogram_summary(\"biases\", biases['bc1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we've finished defining what our graph looks like.  To start running the graph we've built, we need to create a TensorFlow session and initialize the variables in our graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "writer = tf.train.SummaryWriter(\"./logs\", sess.graph)\n",
    "\n",
    "init_op = tf.initialize_all_variables()    \n",
    "# we need init_local_op step only on tensorflow 0.10rc due to a regression from 0.9\n",
    "# https://github.com/tensorflow/models/pull/297\n",
    "init_local_op = tf.initialize_local_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using TensorFlow input queues to feed our graph with each new batch of training data, until all the data is considered.\n",
    "\n",
    "Each image in the training set is used to train our model `TRAIN_EPOCHS` times.  The input queue runner retrieves the next batch of training images and labels, feeds these into the `x_` and `y_` placeholders.  We then run `train_step` to execute one step of our training.  Since TensorFlow has a complete view of our graph dependencies, it runs all the previous dependent nodes of the graph before `train_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with sess.as_default():\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    sess.run(init_local_op) # we need this only with tensorflow 0.10rc\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    print(\" == Start training == \")\n",
    "    \n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            step += 1\n",
    "            x, y = sess.run([train_image_batch, train_label_batch])\n",
    "            train_step.run(feed_dict={keep_prob: 0.75, x_: x, y_: y})\n",
    "\n",
    "            if step % TRAIN_BATCH_SIZE == 0:\n",
    "                x, y = sess.run([valid_image_batch, valid_label_batch])\n",
    "                conv_summary, relu_summary = generate_image_summary(x_, weights, biases, step, image_size)\n",
    "                result = sess.run([cost_summary, accuracy_summary, accuracy, conv_summary, relu_summary, w_summary, \n",
    "                                   b_summary], feed_dict={keep_prob: 1.0, x_: x, y_: y})\n",
    "\n",
    "                cost_summary_str = result[0]\n",
    "                accuracy_summary_str = result[1]\n",
    "                acc = result[2]\n",
    "                conv_summary_str = result[3]\n",
    "                relu_summary_str = result[4]\n",
    "                w_summary_str = result[5]\n",
    "                b_summary_str = result[6]\n",
    "\n",
    "                # write summaries for viewing in Tensorboard\n",
    "                writer.add_summary(accuracy_summary_str, step)\n",
    "                writer.add_summary(cost_summary_str, step)\n",
    "                writer.add_summary(conv_summary_str, step)\n",
    "                writer.add_summary(relu_summary_str, step)\n",
    "                writer.add_summary(w_summary_str, step)\n",
    "                writer.add_summary(b_summary_str, step)\n",
    "\n",
    "                print(\"Accuracy at step %s: %s\" % (step, acc))    \n",
    "                    \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        x, y = sess.run([valid_image_batch, valid_label_batch])\n",
    "        result = sess.run([accuracy], feed_dict={keep_prob: 1.0, x_: x, y_: y})\n",
    "        \n",
    "        print(\"Validation accuracy: %s\" % result[0])\n",
    "\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()    \n",
    "        \n",
    "print(\"You done! (☞ﾟヮﾟ)☞\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
